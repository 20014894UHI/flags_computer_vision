{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4910af16-804f-4c24-955c-6d188027f302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved at: D:/FlagDetectionDatasets/Augmentation/Switch_class_in_images_sets/target32\\F_pipeline_steps_000328.PNG\n"
     ]
    }
   ],
   "source": [
    "# A_Aug_Swap_Flag_Class_over_Flag_Class_with_BG_Fill.jpynb\n",
    "\"\"\"\n",
    "# Include in set for dissertation \n",
    "\n",
    "Script for Merging Flag Classes in Images with Background Filling and Overlay Replacement\n",
    "This script processes a dataset of images and performs the following steps:\n",
    "1. - Identifies the bounding box of a target class in the target image.\n",
    "   - Fills the bounding box with a background color sampled from its surroundings.\n",
    "   - Applies optional padding to ensure smooth blending.\n",
    "\n",
    "2. - Extracts a region of interest (ROI) of a specified class from a source image.\n",
    "    - Aligns the ROI with the target bounding box in the target image.\n",
    "   - Blends the extracted ROI with the target image using a binary mask and Gaussian smoothing.\n",
    "   - Supports custom alignment options (\"top-left\" or \"top-right\") and padding - for flagpole on left or right.\n",
    "\n",
    "3. - Creates a grid of images showing pipeline stages for debugging and demonstration:\n",
    "        - Source image\n",
    "        - Target image\n",
    "        - Target image with background filling\n",
    "        - Final blended image with overlay\n",
    "\n",
    "4.  - Updates YOLO- annotations for the target image to reflect the new overlay.\n",
    "\n",
    "Global Variables:\n",
    "- `merge_class_from`: Path to the folder containing source images.\n",
    "- `merge_class_into`: Path to the folder containing target images.\n",
    "- `merge_fldr`: Path to save processed images and debugging outputs.\n",
    "- `class_to_take`: Source class ID to be overlaid on the target class.\n",
    "- `class_to_replace`: Target class ID to be replaced.\n",
    "- `scale_factor`: Scaling factor for resizing the overlay.\n",
    "- `x_offset`: Horizontal offset for overlay alignment.\n",
    "- `edge_blend_width`: Width of the blending edge.\n",
    "- `threshold`: Intensity threshold for binary mask creation.\n",
    "- `align`: Alignment for overlay positioning (\"top-left\" or \"top-right\").\n",
    "- `padding`: Padding values for bounding box filling (top, right, bottom, left).\n",
    "\n",
    "Key Functions:\n",
    "- `create_merged_filename`: Generates filenames for the merged images and annotations.\n",
    "- `visualize_pipeline_steps`: Saves a grid visualization of the pipeline stages.\n",
    "- `fill_target_bbox_with_background`: Fills the bounding box of the target class with sampled background colors.\n",
    "- `overlay_class_region`: Overlays the source ROI onto the target bounding box with alignment, padding, and blending.\n",
    "\n",
    "How to run \n",
    " Check that `merge_class_from`, `merge_class_into`, and `merge_fldr` directories exist and contain the appropriate images and annotations. \n",
    " Then run: process_switch_class(\n",
    "        src_folder=merge_class_from,\n",
    "        tgt_folder=merge_class_into,\n",
    "        merge_folder=merge_fldr,\n",
    "        target_class=class_to_replace,\n",
    "        source_class_id=class_to_take,\n",
    "        scale_factor=scale_factor,\n",
    "        x_offset=x_offset,\n",
    "        flip_src=False,\n",
    "        flip_tgt=False\n",
    "    )\n",
    "\n",
    "\n",
    "### \"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def create_merged_filename(file_name, merge_from_job, source_class_id, merge_to_job, target_class):\n",
    "    \"\"\"\n",
    "    Create a new filename for the merged image or annotation based on user specifications.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The original filename.\n",
    "        merge_from_job (str): Identifier for the source job.\n",
    "        source_class_id (int): The class ID from the source.\n",
    "        merge_to_job (str): Identifier for the target job.\n",
    "        target_class (int): The target class ID being replaced.\n",
    "\n",
    "    Returns:\n",
    "        str: A new filename that follows the specified naming convention.\n",
    "    \"\"\"\n",
    "    base_name, ext = os.path.splitext(file_name)\n",
    "    new_name = (f\"Merge_{merge_from_job}_CLS_{source_class_id}_\"\n",
    "                f\"into_{merge_to_job}_CLS_{target_class}_{base_name}{ext}\")\n",
    "    return new_name\n",
    "\n",
    "\n",
    "def visualize_pipeline_steps(source_image, target_image, background_filled, final_image, file_name):\n",
    "    #, , background_filled, final_image, output_folder, file_name):\n",
    "\n",
    "    \"\"\"\n",
    "    Save a visualization of the pipeline steps as a grid.\n",
    "    \n",
    "    #Args: This didnt work when passing target images with the bounding box \n",
    "    #    target_image (ndarray): Original target image with the bounding box.\n",
    "    #    background_filled (ndarray): Target image with bounding box filled with background color.\n",
    "    #    resized_source (ndarray): Resized source region.\n",
    "    #    final_image (ndarray): Final output image with overlay and blending.\n",
    "    #    file_name (str): Name of the output visualization file.\n",
    "    \n",
    "    Args: Updating to read them from disk \n",
    "        source_path (str): Path to the source image file.\n",
    "        target_path (str): Path to the target image file.\n",
    "        filled_path (str): Path to the background-filled image file.\n",
    "        final_path (str): Path to the final blended image file.\n",
    "        file_name (str): Name of the output visualization file.\n",
    "    \"\"\"\n",
    "    # Read images from paths instead \n",
    "    source_image = cv2.imread(os.path.join(merge_fldr,source_image))\n",
    "    target_image = cv2.imread(os.path.join(merge_fldr, target_image))\n",
    "    background_filled = cv2.imread(os.path.join(merge_fldr, background_filled))\n",
    "    final_image = cv2.imread(os.path.join(merge_fldr, final_image))\n",
    "    \n",
    "    #target_image = cv2.resize(target_image, (source_image.shape[1], source_image.shape[0]))\n",
    "    #background_filled = cv2.resize(background_filled, (source_image.shape[1], source_image.shape[0]))\n",
    "    #final_image = cv2.resize(final_image, (source_image.shape[1], source_image.shape[0]))\n",
    "    \n",
    "    images = [source_image, target_image, background_filled, final_image]\n",
    "    titles = [\"1. Brightenned source image from which to extract flag\", \"2. Target image on which to overlay flag\", \"3. Background filled target\", \"4. Final blended output with enlarged flag swapped in\"] \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "    output_path = os.path.join(merge_fldr, f\"F_pipeline_steps_{file_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"Visualization saved at: {output_path}\")\n",
    "\n",
    "\n",
    "def fill_target_bbox_with_background(tgt_image, tgt_annotations, target_class, padding, x_offset):\n",
    "    \"\"\"\n",
    "    Fill the bounding box for the target class with the background color sampled from surrounding areas,\n",
    "    applying directional padding.\n",
    "\n",
    "    Args:\n",
    "        tgt_image (ndarray): The target image.\n",
    "        tgt_annotations (list): List of annotations (YOLO format).\n",
    "        target_class (int): The class ID of the target bounding box.\n",
    "        padding (tuple): Padding for (top, right, bottom, left) directions.\n",
    "        x_offset (int): Horizontal offset applied to the bounding box.\n",
    "\n",
    "    Returns:\n",
    "        tgt_image (ndarray): The updated image with the target bounding box filled.\n",
    "    \"\"\"\n",
    "    top_pad, right_pad, bottom_pad, left_pad = padding\n",
    "    image_height, image_width = tgt_image.shape[:2]\n",
    "    target_bbox = None\n",
    "\n",
    "    for annotation in tgt_annotations:\n",
    "        class_id, x_center, y_center, width, height = annotation\n",
    "        if int(class_id) == target_class:\n",
    "            # Apply x_offset to background fill or the overlay not both\n",
    "\n",
    "            #x_min = int((x_center - width / 2) * image_width) + x_offset\n",
    "            #x_max = int((x_center + width / 2) * image_width) + x_offset\n",
    "\n",
    "            x_min = int((x_center - width / 2) * image_width)  #+ x_offset\n",
    "            y_min = int((y_center - height / 2) * image_height)\n",
    "            x_max = int((x_center + width / 2) * image_width)  #+ x_offset\n",
    "            y_max = int((y_center + height / 2) * image_height)\n",
    "\n",
    "            # Ensure coordinates are within bounds\n",
    "            x_min = max(0, x_min - left_pad)\n",
    "            y_min = max(0, y_min - top_pad)\n",
    "            x_max = min(image_width, x_max + right_pad)\n",
    "            y_max = min(image_height, y_max + bottom_pad)\n",
    "\n",
    "            # Sample regions outside the bounding box\n",
    "            top = tgt_image[max(0, y_min - top_pad):y_min, x_min:x_max]\n",
    "            bottom = tgt_image[y_max:min(image_height, y_max + bottom_pad), x_min:x_max]\n",
    "            left = tgt_image[y_min:y_max, max(0, x_min - left_pad):x_min]\n",
    "            right = tgt_image[y_min:y_max, x_max:min(image_width, x_max + right_pad)]\n",
    "            \n",
    "            # Combine the mean colors from sampled regions\n",
    "            sampled_colors = []\n",
    "            if top.size > 0:\n",
    "                sampled_colors.append(top.mean(axis=(0, 1)))\n",
    "            if bottom.size > 0:\n",
    "                sampled_colors.append(bottom.mean(axis=(0, 1)))\n",
    "            if left.size > 0:\n",
    "                sampled_colors.append(left.mean(axis=(0, 1)))\n",
    "            if right.size > 0:\n",
    "                sampled_colors.append(right.mean(axis=(0, 1)))\n",
    "\n",
    "            if not sampled_colors:\n",
    "                print(\"Sampling failed for bounding box. Skipping background filling.\")\n",
    "                continue\n",
    "\n",
    "            # Calculate the average color from all sampled regions\n",
    "            background_color = np.mean(sampled_colors, axis=0).astype(np.uint8)\n",
    "\n",
    "            # Fill the bounding box with the computed background color\n",
    "            tgt_image[y_min:y_max, x_min:x_max] = background_color\n",
    "            target_bbox = (x_min, y_min, x_max, y_max)\n",
    "            break\n",
    "\n",
    "    if target_bbox is None:\n",
    "        print(\"No bounding box found for target class. Skipping background filling.\")\n",
    "\n",
    "    return tgt_image\n",
    "\n",
    "\n",
    "def overlay_class_region(\n",
    "    file_name, src_image, src_annotations, source_class_id, tgt_image, tgt_annotations, \n",
    "    target_class, scale_factor, x_offset, edge_blend_width, \n",
    "    threshold, padding, align, visualize_once=True, debug_folder=None):\n",
    "    \"\"\"\n",
    "    Overlay the source class region onto the target bounding box with alignment and optional padding.\n",
    "\n",
    "    Args:\n",
    "        src_image (ndarray): The source image.\n",
    "        src_annotations (list): List of annotations for the source image.\n",
    "        source_class_id (int): The class ID to extract from the source image.\n",
    "        tgt_image (ndarray): The target image.\n",
    "        tgt_annotations (list): List of annotations for the target image.\n",
    "        target_class (int): The target class ID to replace.\n",
    "        scale_factor (float): Scaling factor for resizing the extracted region.\n",
    "        x_offset (int): Horizontal offset for overlay placement.\n",
    "        edge_blend_width (int): Width of the edges to apply blending.\n",
    "        threshold (int): Intensity threshold for binary mask creation.\n",
    "        debug_folder (str): Folder path to save debugging visualizations. If None, debugging is skipped.\n",
    "        align (str): Alignment option for the overlay (\"top-left\" or \"top-right\").\n",
    "        padding (tuple): Padding to apply (top, right, bottom, left).\n",
    "        visualize_once (bool): If True, visualizations will only occur for the first file processed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated target image, new annotation for the overlaid class, and updated annotations.\n",
    "    \"\"\"\n",
    "    image_height, image_width = src_image.shape[:2]\n",
    "    src_roi = None\n",
    "    new_annotation = None\n",
    "    # Flag to track if it's the first file being processed\n",
    "    global visualized\n",
    "    if 'visualized' not in globals():\n",
    "        visualized = False\n",
    "        \n",
    "    # Extract the region of interest (ROI) for the source class\n",
    "    for annotation in src_annotations:\n",
    "        class_id, x_center, y_center, width, height = annotation\n",
    "        if int(class_id) == source_class_id:\n",
    "            box_width = int(width * image_width)\n",
    "            box_height = int(height * image_height)\n",
    "            x_center_px = int(x_center * image_width)\n",
    "            y_center_px = int(y_center * image_height)\n",
    "\n",
    "            x_min = max(0, x_center_px - box_width // 2)\n",
    "            y_min = max(0, y_center_px - box_height // 2)\n",
    "            x_max = min(image_width, x_center_px + box_width // 2)\n",
    "            y_max = min(image_height, y_center_px + box_height // 2)\n",
    "\n",
    "            src_roi = src_image[y_min:y_max, x_min:x_max]\n",
    "            break\n",
    "\n",
    "    if src_roi is None:\n",
    "        print(\"No matching class found in the source image.\")\n",
    "        return tgt_image, new_annotation\n",
    "\n",
    "    for annotation in tgt_annotations:\n",
    "        \n",
    "        class_id, x_center, y_center, width, height = annotation\n",
    "        if int(class_id) == target_class:\n",
    "            # Calculate the center and size of the target bounding box\n",
    "            tgt_x_center = int(x_center * tgt_image.shape[1])  \n",
    "            tgt_y_center = int(y_center * tgt_image.shape[0])\n",
    "            tgt_width = int(width * tgt_image.shape[1] * scale_factor) \n",
    "            tgt_height = int(height * tgt_image.shape[0] * scale_factor)\n",
    "\n",
    "            # Align the overlay based on the alignment option\n",
    "            if align == \"top-left\":\n",
    "                tgt_x_min = max(0, tgt_x_center - tgt_width // 2 + x_offset)\n",
    "                tgt_y_min = max(0, tgt_y_center - tgt_height // 2)\n",
    "            elif align == \"top-right\":\n",
    "                tgt_x_min = max(0, tgt_x_center - tgt_width // 2 + x_offset)\n",
    "                tgt_y_min = max(0, tgt_y_center - tgt_height // 2)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid alignment option. Choose 'top-left' or 'top-right'.\")\n",
    "\n",
    "            # Apply padding\n",
    "            top_pad, right_pad, bottom_pad, left_pad = padding\n",
    "            tgt_x_min = max(0, tgt_x_min - left_pad)\n",
    "            tgt_y_min = max(0, tgt_y_min - top_pad)\n",
    "            tgt_x_max = min(tgt_image.shape[1], tgt_x_min + tgt_width + right_pad)\n",
    "            tgt_y_max = min(tgt_image.shape[0], tgt_y_min + tgt_height + bottom_pad)\n",
    "\n",
    "            # Resize the source ROI to fit the target bounding box\n",
    "            resized_src_roi = cv2.resize(src_roi, (tgt_x_max - tgt_x_min, tgt_y_max - tgt_y_min))\n",
    "            # cv2.imwrite(os.path.join(debug_folder, f\"D_1_resized_src_roi_{file_name}\"), (resized_src_roi * 255).astype(np.uint8))\n",
    "\n",
    "            if visualize_once and not visualized:\n",
    "            # Preview the source roi\n",
    "                #plt.imshow(src_roi, cmap='gray')  # 'gray' colormap for single-channel images\n",
    "                #plt.title(\"Source ROI\")\n",
    "                #plt.axis(\"off\")  \n",
    "                #plt.show()\n",
    "\n",
    "                # Correct colour space \n",
    "                src_roi_rgb = cv2.cvtColor(src_roi, cv2.COLOR_BGR2RGB)\n",
    "                plt.imshow(src_roi_rgb)\n",
    "                plt.title(\"Source ROI\")\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "                 \n",
    "                # Preview the resized roi\n",
    "                resized_src_roi_rgb = cv2.cvtColor(resized_src_roi, cv2.COLOR_BGR2RGB)\n",
    "                #plt.imshow(resized_src_roi, cmap='gray') \n",
    "                plt.imshow(resized_src_roi_rgb)\n",
    "                plt.title(\"Resized source ROI to fit target bounding box\")\n",
    "                plt.axis(\"off\")  \n",
    "                plt.show()\n",
    "                \n",
    "            # Create a binary mask for the flag\n",
    "            gray_roi = cv2.cvtColor(resized_src_roi, cv2.COLOR_BGR2GRAY)\n",
    "            _, binary_mask = cv2.threshold(gray_roi, threshold, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "            # Dynamic thresholding using Otsu' method - red part of flag was retained \n",
    "            #_, binary_mask = cv2.threshold(gray_roi, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            \n",
    "            # Note: re dynamic threhsold with Otsu's method above\n",
    "            #       Otsu's method dynamically calculates a global threshold based on the image histogram. \n",
    "            #       When applied to a Red/Yellow flag, it obliterates the yellow part of the flag; the thresholding likely does not adequately distinguish \n",
    "            #       between the yellow flag and the background in grayscale. \n",
    "            \n",
    "            binary_mask = binary_mask.astype(np.float32)\n",
    "\n",
    "            #  Smooth the binary mask using Gaussian blur\n",
    "            #  Kernel size (5, 5) controls the extent of smoothing.\n",
    "            #  sigmaX determines the standard deviation of the Gaussian kernel. Lower values retain more detail, higher values smooth more aggressively.\n",
    "            #  Expected improvements: Smoother Transitions: The blurred mask ensures that the blending region doesn't have jagged or harsh edges.\n",
    "            #  More Natural Look: Morphological operations remove artifacts and make the edges more consistent.\n",
    "\n",
    "            binary_mask = cv2.GaussianBlur(binary_mask, (5, 5), sigmaX=1)\n",
    "\n",
    "            # Compute the gradient magnitude\n",
    "            #sobelx = cv2.Sobel(binary_mask, cv2.CV_64F, 1, 0, ksize=3)\n",
    "            #sobely = cv2.Sobel(binary_mask, cv2.CV_64F, 0, 1, ksize=3)\n",
    "            #gradient_magnitude = cv2.magnitude(sobelx, sobely)\n",
    "            # Normalize the gradient image\n",
    "            #gradient_magnitude = cv2.normalize(gradient_magnitude, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "            # Threshold the gradient magnitude\n",
    "            #_, binary_mask = cv2.threshold(gradient_magnitude, 50, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "            if visualize_once and not visualized:\n",
    "                # Preview the smoothed binary mask\n",
    "                plt.imshow(binary_mask, cmap='gray')  # 'gray' colormap for single-channel images\n",
    "                plt.title(\"Smoothed binary mask\")\n",
    "                plt.axis(\"off\")  \n",
    "                plt.show()\n",
    "            \n",
    "            # Binary mask: Shows the flag pixels detected within the bounding box. \n",
    "            # Helps verify the threshold's effectiveness in capturing only the desired region.\n",
    "            # cv2.imwrite(os.path.join(debug_folder, f\"D_2_binary_mask_smoothed_{file_name}\"), (binary_mask * 255).astype(np.uint8))\n",
    "\n",
    "            # Invert the binary mask to focus on the external background\n",
    "            # The flag region (identified as 1 in the mask) becomes 0, and the background becomes 1. \n",
    "            # Allows the blending logic to target the background for blending, while preserving the flag interior.\n",
    "            # The flag interior remains untouched, as its corresponding pixels in the mask are set to 0. \n",
    "            # Highlights the external region around the flag for blending and ensures that the focus is outside the flag.\n",
    "\n",
    "            binary_mask = 1 - binary_mask\n",
    "\n",
    "            if visualize_once and not visualized:\n",
    "                # Preview the inverted binary mask\n",
    "                plt.imshow(binary_mask, cmap='gray')  \n",
    "                plt.title(\"Inverted binary mask\")\n",
    "                plt.axis(\"off\")  \n",
    "                plt.show()\n",
    "\n",
    "            # cv2.imwrite(os.path.join(debug_folder, f\"D_3_inverted_mask_{file_name}\"), (binary_mask * 255).astype(np.uint8))\n",
    "              \n",
    "            # Expand the inverted mask slightly for better edge blending\n",
    "            # Mask Expansion: Dilation ensures that the edges of the background around the flag blend better with the target image.\n",
    "\n",
    "            # Improvements to make: \n",
    "            # 1. Adjustable mask expansion - Allow a parameter to control the kernel size:\n",
    "            #        kernel = np.ones((kernel_size, kernel_size), np.uint8)  # Pass kernel_size as an argument\n",
    "            #        binary_mask = cv2.dilate(binary_mask, kernel, iterations=1)\n",
    "    \n",
    "            #kernel = np.ones((3, 3), np.uint8)  # Used for original image sets that worked out well \n",
    "            kernel = np.ones((1, 1), np.uint8)  # Adjust kernel size if needed\n",
    "            #binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel, iterations=4)  # Close small holes\n",
    "            #binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel, iterations=1)   # Remove noise\n",
    "            binary_mask = cv2.dilate(binary_mask, kernel, iterations=1)\n",
    "\n",
    "            # Dilated Mask: Displays the expanded blending region around the flag and ensures smooth transitions at the edges\n",
    "            # cv2.imwrite(os.path.join(debug_folder, f\"D_4_dilated_mask_{file_name}\"), (binary_mask * 255).astype(np.uint8))\n",
    "   \n",
    "            if visualize_once and not visualized:\n",
    "                # Preview the inverted binary mask\n",
    "                plt.imshow(binary_mask, cmap='gray')  # 'gray' colormap for single-channel images\n",
    "                plt.title(\"Dilated mask\")\n",
    "                plt.axis(\"off\")  \n",
    "                plt.show()\n",
    "\n",
    "                # cv2.imwrite(os.path.join(debug_folder, f\"D_5_blended_region_{file_name}\"), (blended_region * 255).astype(np.uint8))\n",
    "                #print(f\"Resized Source ROI Shape: {resized_src_roi.shape}\")\n",
    "                #print(f\"Resized Source ROI Pixel Range: {resized_src_roi.min()} to {resized_src_roi.max()}\")\n",
    "    \n",
    "                #print(f\"Binary Mask Shape: {binary_mask.shape}\")\n",
    "                #print(f\"Binary Mask Pixel Range: {binary_mask.min()} to {binary_mask.max()}\")\n",
    "    \n",
    "                #print(f\"Blended Region Coordinates: ({tgt_x_min}, {tgt_y_min}) to ({tgt_x_max}, {tgt_y_max})\")\n",
    "                #print(f\"Target Image Region Shape: {tgt_image[tgt_y_min:tgt_y_max, tgt_x_min:tgt_x_max].shape}\")\n",
    "    \n",
    "                #print(f\"Blended Region Data Type: {blended_region.dtype}\")\n",
    "                #print(f\"Blended Region Pixel Range: {blended_region.min()} to {blended_region.max()}\")\n",
    "          \n",
    "\n",
    "            # Blend the flag's background with the target image\n",
    "            for c in range(3):\n",
    "                tgt_image[tgt_y_min:tgt_y_max, tgt_x_min:tgt_x_max, c] = (\n",
    "                    resized_src_roi[:, :, c] * binary_mask +\n",
    "                    tgt_image[tgt_y_min:tgt_y_max, tgt_x_min:tgt_x_max, c] * (1 - binary_mask)\n",
    "            )\n",
    "\n",
    "            # Visualise the blended region - only used for visualusation \n",
    "            blended_region = tgt_image[tgt_y_min:tgt_y_max, tgt_x_min:tgt_x_max]\n",
    "\n",
    "            if visualize_once and not visualized:\n",
    "                # Preview the blended region \n",
    "                plt.imshow(cv2.cvtColor(blended_region, cv2.COLOR_BGR2RGB))\n",
    "                plt.title(\"Blended Region\")\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "            visualized = True  # Ensure visualizations occur only once\n",
    "\n",
    "            # Update annotations for the new overlay\n",
    "            #new_annotation = [\n",
    "            #    source_class_id,\n",
    "            #    round(tgt_x_center / tgt_image.shape[1], 6),\n",
    "            #    round(tgt_y_center / tgt_image.shape[0], 6),\n",
    "            #    round(tgt_width / tgt_image.shape[1], 6),\n",
    "            #    round(tgt_height / tgt_image.shape[0], 6),\n",
    "            #]\n",
    "\n",
    "\n",
    "            # Explicitly recalculate tgt_x_center and tgt_y_center as the midpoint of the bounding box to ensure consistency between the overlay and the annotation.\n",
    "            new_annotation = [\n",
    "                source_class_id,\n",
    "                round((tgt_x_min + tgt_width // 2) / tgt_image.shape[1], 6),  # x_center with x-offset applied\n",
    "                round((tgt_y_min + tgt_height // 2) / tgt_image.shape[0], 6), # y_center\n",
    "                round(tgt_width / tgt_image.shape[1], 6),                     # width\n",
    "                round(tgt_height / tgt_image.shape[0], 6),                    # height\n",
    "            ]\n",
    "\n",
    "            #print(f\"Target Bounding Box (Pixels): x_min={tgt_x_min}, y_min={tgt_y_min}, x_max={tgt_x_max}, y_max={tgt_y_max}\")\n",
    "            #print(f\"New Annotation (YOLO Format): {new_annotation}\")\n",
    "            #print(f\"Target Image Dimensions: {tgt_image.shape[1]} x {tgt_image.shape[0]}\")\n",
    "\n",
    "            break\n",
    "\n",
    "    # Remove the target class annotation\n",
    "    tgt_annotations = [annotation for annotation in tgt_annotations if int(annotation[0]) != target_class]\n",
    "\n",
    "    return tgt_image, new_annotation, tgt_annotations\n",
    "\n",
    "    \n",
    "def process_switch_class(src_folder, tgt_folder, merge_folder, target_class, source_class_id, scale_factor, x_offset, flip_src=False, flip_tgt=False):\n",
    "    \n",
    "    countprocessed=0\n",
    "    \n",
    "    if not os.path.exists(merge_folder):\n",
    "        os.makedirs(merge_folder)\n",
    "\n",
    "    for file_name in os.listdir(src_folder):\n",
    "        if file_name.lower().endswith('.png'):\n",
    "            src_image = cv2.imread(os.path.join(src_folder, file_name))\n",
    "            if flip_src:\n",
    "                src_image = cv2.flip(src_image, 1)\n",
    "            src_annotation_path = os.path.join(src_folder, os.path.splitext(file_name)[0] + \".txt\")\n",
    "\n",
    "            if not os.path.exists(src_annotation_path):\n",
    "                #print(f\"No annotations for {file_name} in source ({src_folder}). Skipping.\")\n",
    "                continue\n",
    "\n",
    "            with open(src_annotation_path, 'r') as f:\n",
    "                src_annotations = [list(map(float, line.split())) for line in f]\n",
    "\n",
    "            tgt_image_path = os.path.join(tgt_folder, file_name)\n",
    "            tgt_image = cv2.imread(tgt_image_path)\n",
    "            if flip_tgt:\n",
    "                tgt_image = cv2.flip(tgt_image, 1)\n",
    "            tgt_annotation_path = os.path.join(tgt_folder, os.path.splitext(file_name)[0] + \".txt\")\n",
    "\n",
    "            if not os.path.exists(tgt_annotation_path) or tgt_image is None:\n",
    "                #print(f\"No target annotations or image for {file_name} in {tgt_folder}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            with open(tgt_annotation_path, 'r') as f:\n",
    "                tgt_annotations = [list(map(float, line.split())) for line in f]\n",
    "\n",
    "            countprocessed = countprocessed +1\n",
    "            # Save the first original target image for visualization\n",
    "            if countprocessed == 1:\n",
    "                cv2.imwrite(os.path.join(merge_folder, f\"A_Source_image_{file_name}\"), src_image)\n",
    "                cv2.imwrite(os.path.join(merge_folder, f\"B_Original_target_{file_name}\"), tgt_image)\n",
    "\n",
    "            # Fill target bounding box with background\n",
    "            # background_filled_image = fill_target_bbox_with_background(tgt_image, tgt_annotations, target_class)\n",
    "            # Selective padding - send 0 for the left or right depending on the relative position of the flagpole - could parameterize \n",
    "            background_filled_image = fill_target_bbox_with_background(\n",
    "                tgt_image=tgt_image,\n",
    "                tgt_annotations=tgt_annotations,\n",
    "                target_class=target_class,\n",
    "                padding=padding,  \n",
    "                x_offset=x_offset\n",
    "            )\n",
    "            \n",
    "            # Save the first background-filled target image for visualization\n",
    "            # if countprocessed == 1:\n",
    "            cv2.imwrite(os.path.join(merge_folder, f\"C_Target_Image_with_flag_filled_{file_name}\"), background_filled_image)\n",
    "\n",
    "            # Overlay the source class onto the target class\n",
    "            # updated_image, new_annotation, updated_annotations = overlay_class_region(\n",
    "            #   src_image, src_annotations, source_class_id, background_filled_image, tgt_annotations, target_class, scale_factor, x_offset, edge_blend_width=30, debug_folder=merge_folder)\n",
    "\n",
    "            updated_image, new_annotation, updated_annotations = overlay_class_region(\n",
    "                file_name,\n",
    "                src_image=src_image,\n",
    "                src_annotations=src_annotations,\n",
    "                source_class_id=source_class_id,\n",
    "                tgt_image=background_filled_image,\n",
    "                tgt_annotations=tgt_annotations,\n",
    "                target_class=target_class,\n",
    "                scale_factor = scale_factor,      \n",
    "                x_offset = x_offset,      \n",
    "                edge_blend_width = edge_blend_width,\n",
    "                threshold = threshold,    \n",
    "                align=align,  \n",
    "                padding=padding,\n",
    "                debug_folder=merge_folder\n",
    "            )\n",
    "\n",
    "            # Overlay the source class onto the target class\n",
    "            #updated_image, new_annotation, updated_annotations = overlay_class_region_with_corrected_blending(\n",
    "            #    src_image, src_annotations, source_class_id, background_filled_image, tgt_annotations, target_class, scale_factor, x_offset, edge_blend_width=10)\n",
    "\n",
    "            if countprocessed == 1:\n",
    "                # Save the first overlay image for visualization\n",
    "                cv2.imwrite(os.path.join(merge_folder, f\"E_overlay_result_{file_name}\"), updated_image)\n",
    "\n",
    "            # Create a new filename and save the updated image\n",
    "            new_filename = create_merged_filename(file_name, merge_from_job, source_class_id, merge_to_job, target_class)\n",
    "            output_path = os.path.join(merge_folder, new_filename)\n",
    "            cv2.imwrite(output_path, updated_image)\n",
    "\n",
    "            if countprocessed == 1:\n",
    "                # visualize_pipeline_steps(target_image=tgt_image,background_filled=background_filled_image,resized_source=resized_src_roi,final_image=updated_image,file_name=file_name)\n",
    "                # visualize_pipeline_steps(source_image=src_image,target_image=tgt_image,background_filled=background_filled_image, file_name=file_name) \n",
    "                visualize_pipeline_steps(source_image=f\"A_Source_image_{file_name}\",\n",
    "                                             target_image=f\"B_Original_target_{file_name}\",\n",
    "                                             background_filled=f\"C_Target_Image_with_flag_filled_{file_name}\", \n",
    "                                             final_image = f\"E_overlay_result_{file_name}\",\n",
    "                                             file_name=file_name) \n",
    "           \n",
    "            # Save the updated annotation file\n",
    "            new_annotation_path = os.path.splitext(output_path)[0] + \".txt\"\n",
    "            with open(new_annotation_path, 'w') as f:\n",
    "                for annotation in updated_annotations:\n",
    "                    formatted_annotation = [int(annotation[0])] + [f\"{coord:.6f}\" for coord in annotation[1:]]\n",
    "                    f.write(' '.join(map(str, formatted_annotation)) + '\\n')\n",
    "                if new_annotation:\n",
    "                    formatted_annotation = [int(new_annotation[0])] + [f\"{coord:.6f}\" for coord in new_annotation[1:]]\n",
    "                    f.write(' '.join(map(str, formatted_annotation)) + '\\n')\n",
    "\n",
    "#merge_class_from = \"D:/FlagDetectionDatasets/Augmentation/Switch_class_in_images_sets/Switch_class_1_from_43_into_88/Switch_flag_out_of_43_illum\"\n",
    "#merge_class_into = \"D:/FlagDetectionDatasets/Augmentation/Switch_class_in_images_sets/Switch_class_1_from_43_into_88/Switch_flag_into_88\"\n",
    "#merge_fldr = \"D:/FlagDetectionDatasets/Augmentation/Switch_class_in_images_sets/Switch_class_1_from_43_into_88/Merge_43_to_88_with_fillC\"\n",
    "\n",
    "merge_class_from = 'D:/FlagDetectionDatasets/Augmentation/Switch_class_in_images_sets/input'\n",
    "merge_class_into = 'D:/FlagDetectionDatasets/Augmentation/Switch_class_in_images_sets/target'\n",
    "merge_fldr = 'D:/FlagDetectionDatasets/Augmentation/Switch_class_in_images_sets/target32'\n",
    "merge_from_job = '30' # Just used in naming \n",
    "merge_to_job = '24'   # Just used in naming \n",
    "\n",
    "class_to_take = 1     # Source class to be merged over the target class in the new image \n",
    "class_to_replace = 0  # Target class to be replaced \n",
    "scale_factor = 1.2    # Scaling factor for the source region\n",
    "x_offset = 20 #-80 #20         # Horizontal offset for overlay placement - fine tune differently \n",
    "edge_blend_width = 20  # Width of edge blending\n",
    "threshold = 190 # 190 #245       # 190 Intensity threshold for binary mask  \n",
    "debug_folder = merge_fldr  # Path to save debugging visualizations\n",
    "align = 'top-left'  #'top-left' # Alignment option (\"top-left\" or \"top-right\")\n",
    "padding=(5, 5, 5, 0)   # Padding: (top, right, bottom, left) #padding=(0, 5, 5, 5)\n",
    "#border_size=10,      # Sample colour regions \n",
    "\n",
    "# Ensure debug folder exists\n",
    "#if not os.path.exists(debug_folder):\n",
    "#    os.makedirs(debug_folder)\n",
    "\n",
    "process_switch_class(merge_class_from, merge_class_into, merge_fldr, class_to_replace, class_to_take, scale_factor, x_offset, flip_src=False, flip_tgt=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a64221c-b22e-47df-a545-164360f0ee3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
